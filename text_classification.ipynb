{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_classification.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1ZHGSSQbAQoCc_QKhaNw99Oh4zldsyr40","authorship_tag":"ABX9TyPd0Ob7jOE+Qzo0RDFkO2z6"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WD_FPz9zOz6r"},"source":["# Text Classfication\n","\n","The goal of text classification is to classify a given piece of text(s) into a predefined categories. A classic example is classifiying spam and non-spam emails. Hence, text classification is a supervised learning task.\n","\n","In this notebook, we will create a text classification framework."]},{"cell_type":"code","metadata":{"id":"_WxItjMRf_uR"},"source":["from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn import decomposition, ensemble\n","\n","import pandas, xgboost, numpy, textblob, string\n","from keras.preprocessing import text, sequence\n","from keras import layers, models, optimizers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lwbjrKtUQv-z"},"source":["## Data\n","\n","We will use the \"amazon reviews\" dataset https://gist.github.com/kunalj101/ad1d9c58d338e20d09ff26bcc06c4235. "]},{"cell_type":"code","metadata":{"id":"WyzrzEtjgv9l"},"source":["# load the dataset\n","data = open('drive/MyDrive/amazon_reviews.txt').read()\n","labels, texts = [], []\n","for i, line in enumerate(data.split(\"\\n\")):\n","    content = line.split()\n","    labels.append(content[0])\n","    texts.append(\" \".join(content[1:]))\n","\n","# create a dataframe using texts and lables\n","trainDF = pandas.DataFrame()\n","trainDF['text'] = texts\n","trainDF['label'] = labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9NBMRVOg_oV"},"source":["# split the dataset into training and validation datasets \n","train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n","\n","# label encode the target variable \n","encoder = preprocessing.LabelEncoder()\n","train_y = encoder.fit_transform(train_y)\n","valid_y = encoder.fit_transform(valid_y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OC3JLwg5ZiAC"},"source":["## Feature Engineering\n","\n","We will convert raw text data into feature vectors, using many aproaches."]},{"cell_type":"markdown","metadata":{"id":"7oKlOhUEZ1zT"},"source":["### Count Vectors\n","\n","Count Vector notation creates a matrix from the dataset, where each row represents a document from the corpus, each column a term, and each cell the frequency count of a term in a document."]},{"cell_type":"code","metadata":{"id":"BDCMERcIrZid"},"source":["# create a count vectorizer object \n","count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n","count_vect.fit(trainDF['text'])\n","\n","# transform the training and validation data using count vectorizer object\n","xtrain_count =  count_vect.transform(train_x)\n","xvalid_count =  count_vect.transform(valid_x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zu2E6Mxtppcy"},"source":["### TF-IDF Vectors\n","\n","TF-IDF score represents the relative importance of a term in a document and the corpus. TF-IDF consists of two components: normalised Term Frequency (TF), and Inverse Document Frequency (IDF) - computed as the logarithm of the number of documents in the corpus divided by the number of documents where the term appears.\n","\n","TF-IDF vectors can be generated at different levels of input:\n","1. Word Level: Representing the scores of every term.\n","2. N-gram Level: Representing the scores of N terms combined together.\n","3. Character Level: Representing character level n-grams."]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"fWugbdLTrou3","executionInfo":{"elapsed":19876,"status":"ok","timestamp":1623831118907,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"d8a5cf04-aa8e-42de-8042-7ce908527566"},"source":["# word level tf-idf\n","tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n","tfidf_vect.fit(trainDF['text'])\n","xtrain_tfidf =  tfidf_vect.transform(train_x)\n","xvalid_tfidf =  tfidf_vect.transform(valid_x)\n","\n","# ngram level tf-idf \n","tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n","tfidf_vect_ngram.fit(trainDF['text'])\n","xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n","xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n","\n","# characters level tf-idf\n","tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n","tfidf_vect_ngram_chars.fit(trainDF['text'])\n","xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n","xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"OpumWtxXz2QS"},"source":["### Word Embeddings\n","\n","A word embedding represents words in a dense vector format. The position of a word within the vector space is learned from the input text, and is close to other words that often surround it. E.g., \"that\" is often followed by \"is\", therefore these two words would be close to one another in the vector space.\n","\n","Word Embeddings can be trained on any corpus, but we can also use pre-trained ones, which we will do."]},{"cell_type":"code","metadata":{"id":"YBoMl4KKr-SV"},"source":["# load the pre-trained word-embedding vectors \n","embeddings_index = {}\n","for i, line in enumerate(open('drive/MyDrive/wiki-news-300d-1M.vec')):\n","    values = line.split()\n","    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n","\n","# create a tokenizer \n","token = text.Tokenizer()\n","token.fit_on_texts(trainDF['text'])\n","word_index = token.word_index\n","\n","# convert text to sequence of tokens and pad them to ensure equal length vectors \n","train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n","valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n","\n","# create token-embedding mapping\n","embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q55uCn1V8vey"},"source":["### Text based features\n","\n","The code segments below demonstrate some other features we can use. Note that these features are highly experimental, and should only be used according to a necessary problem statement."]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"blMcY-ZQOimX"},"source":["trainDF['char_count'] = trainDF['text'].apply(len)\n","trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n","trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n","trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n","trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n","trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kPPbSvPOs9m","executionInfo":{"elapsed":287121,"status":"ok","timestamp":1623831496626,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"4a943ed7-10af-4f45-ed1e-fe5bc8e9d213"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","\n","pos_family = {\n","    'noun' : ['NN','NNS','NNP','NNPS'],\n","    'pron' : ['PRP','PRP$','WP','WP$'],\n","    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n","    'adj' :  ['JJ','JJR','JJS'],\n","    'adv' : ['RB','RBR','RBS','WRB']\n","}\n","\n","# function to check and get the part of speech tag count of a words in a given sentence\n","def check_pos_tag(x, flag):\n","    cnt = 0\n","    try:\n","        wiki = textblob.TextBlob(x)\n","        for tup in wiki.tags:\n","            ppo = list(tup)[1]\n","            if ppo in pos_family[flag]:\n","                cnt += 1\n","    except:\n","        pass\n","    return cnt\n","\n","trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n","trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n","trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n","trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n","trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hTnwgXdhRJiM"},"source":["### Topic Models\n","\n","Topic Modelling is a technique which identifies groups of words (topic) from the documents. We will use Latent Dirichlet Allocation (LDA) to generate topic modelling features. LDA is an interative model which starts from a fixed number of topics. The probability distributions over words given by the topics provides a sense of the differing ideas in the documents."]},{"cell_type":"code","metadata":{"id":"9N21lsVJ4LqG"},"source":["# train a LDA Model\n","lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n","X_topics = lda_model.fit_transform(xtrain_count)\n","topic_word = lda_model.components_ \n","vocab = count_vect.get_feature_names()\n","\n","# view the topic models\n","n_top_words = 10\n","topic_summaries = []\n","for i, topic_dist in enumerate(topic_word):\n","    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n","    topic_summaries.append(' '.join(topic_words))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y0r_Po-ESmdM"},"source":["## Model Building\n","\n","We will implement many different classifiers:\n","\n","Naive Bayes Classifier,\n","Linear Classifier,\n","Support Vector Machine,\n","Bagging Models,\n","Boosting Models,\n","Shallow Neural Networks, and\n","Deep Neural Networks which include:\n","  Convolutional Neural Network (CNN),\n","  Long Short Term Model (LSTM),\n","  Gated Recurrent Unit (GRU),\n","  Bidirectional RNN,\n","  Recurrent Convolutional Neural Network (RCNN), and\n","  Other Variants of Deep Neural Networks.\n","\n","  Firstly, we will define a utility function for the classifiers we will use:"]},{"cell_type":"code","metadata":{"id":"nzNtJL3A6HFZ"},"source":["def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n","    # fit the training dataset on the classifier\n","    classifier.fit(feature_vector_train, label)\n","    \n","    # predict the labels on validation dataset\n","    predictions = classifier.predict(feature_vector_valid)\n","    \n","    if is_neural_net:\n","        predictions = predictions.argmax(axis=-1)\n","    \n","    return metrics.accuracy_score(predictions, valid_y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vc2esVkyew7w"},"source":["### Naive Bayes\n","\n","Naive Bayes is a classification model based on Bayes's Theorem, with assumption of independence among predictors."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuw9tZVa6OnM","executionInfo":{"elapsed":198,"status":"ok","timestamp":1623831724932,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"58696b58-2749-4927-9f93-1fdf3160512b"},"source":["# Naive Bayes on Count Vectors\n","accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n","print(\"NB, Count Vectors: \", accuracy)\n","\n","# Naive Bayes on Word Level TF IDF Vectors\n","accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n","print(\"NB, WordLevel TF-IDF: \", accuracy)\n","\n","# Naive Bayes on Ngram Level TF IDF Vectors\n","accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n","print(\"NB, N-Gram Vectors: \", accuracy)\n","\n","# Naive Bayes on Character Level TF IDF Vectors\n","accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n","print(\"NB, CharLevel Vectors: \", accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["NB, Count Vectors:  0.836\n","NB, WordLevel TF-IDF:  0.8512\n","NB, N-Gram Vectors:  0.8464\n","NB, CharLevel Vectors:  0.8112\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6zs4uVZ7nkmK"},"source":["### Logisitc regression\n","\n","Logistic regression measures the relationship between the dependent variable and one or more independent variables by estimating probabilities using a logistic function."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XwbET79l6ku2","executionInfo":{"elapsed":2334,"status":"ok","timestamp":1623831906634,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"fc8f1234-6443-444f-fbd7-61c5f449dd44"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# Linear Classifier on Count Vectors\n","accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n","print(\"LR, Count Vectors: \", accuracy)\n","\n","# Linear Classifier on Word Level TF IDF Vectors\n","accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n","print(\"LR, WordLevel TF-IDF: \", accuracy)\n","\n","# Linear Classifier on Ngram Level TF IDF Vectors\n","accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n","print(\"LR, N-Gram Vectors: \", accuracy)\n","\n","# Linear Classifier on Character Level TF IDF Vectors\n","accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n","print(\"LR, CharLevel Vectors: \", accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["LR, Count Vectors:  0.868\n","LR, WordLevel TF-IDF:  0.8696\n","LR, N-Gram Vectors:  0.8332\n","LR, CharLevel Vectors:  0.8372\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C3HX900yoX2I"},"source":["Support vector machine\n","\n","A support vector machine (SVM) can be used for both classification and regression problems. The model extracts the best possible hyperplane that segregates the two classes."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9qjEAl47Pt4","executionInfo":{"elapsed":24292,"status":"ok","timestamp":1623831979398,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"8ceb07f1-6051-48b0-f57e-a65e9bc899a0"},"source":["# SVM on Ngram Level TF IDF Vectors\n","accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n","print(\"SVM, N-Gram Vectors: \", accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["SVM, N-Gram Vectors:  0.838\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KKlUAimjpLYV"},"source":["### Random forest\n","\n","Random forest models are a type of ensemble algorithm, and more precisely, bagging algorithm."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G2azBsKT7e8Z","executionInfo":{"elapsed":20044,"status":"ok","timestamp":1623832039156,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"6410c522-62d9-45df-9dfa-e72883e28b58"},"source":["# RF on Count Vectors\n","accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n","print(\"RF, Count Vectors: \", accuracy)\n","\n","# RF on Word Level TF IDF Vectors\n","accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n","print(\"RF, WordLevel TF-IDF: \", accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RF, Count Vectors:  0.8248\n","RF, WordLevel TF-IDF:  0.83\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RkI9nyR2ppZ5"},"source":["### Extreme gradient boosting (XGBoost)\n","\n","XGBoost is a boosting model which is another type of ensemble method. It is a meta-algorithm primarily for reducing bias, but also variance in supervised learning, which converts weaker models into stronger ones."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2_w6zCyJ7s-v","executionInfo":{"elapsed":53161,"status":"ok","timestamp":1623832132787,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"3f2f1938-de6b-49a3-be0b-c7a2bd1ab533"},"source":["# Extereme Gradient Boosting on Count Vectors\n","accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n","print(\"Xgb, Count Vectors: \", accuracy)\n","\n","# Extereme Gradient Boosting on Word Level TF IDF Vectors\n","accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n","print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n","\n","# Extereme Gradient Boosting on Character Level TF IDF Vectors\n","accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n","print(\"Xgb, CharLevel Vectors: \", accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Xgb, Count Vectors:  0.808\n","Xgb, WordLevel TF-IDF:  0.7956\n","Xgb, CharLevel Vectors:  0.808\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZGP5KJuBqzuD"},"source":["### Shallow neural network\n","\n","A neural network is a mathematical model designed to mimic biological neurons. These models are used to recognise complex patterns among data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n1iMkJwe8NVZ","executionInfo":{"elapsed":17553,"status":"ok","timestamp":1623832219820,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"df233370-a2d4-435f-dd4d-518e13fb29f3"},"source":["def create_model_architecture(input_size):\n","    # create input layer \n","    input_layer = layers.Input((input_size, ), sparse=True)\n","    \n","    # create hidden layer\n","    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n","    \n","    # create output layer\n","    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n","\n","    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n","    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n","    return classifier \n","\n","classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n","accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n","print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["235/235 [==============================] - 16s 8ms/step - loss: 0.6127\n","NN, Ngram Level TF IDF Vectors 0.5212\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xzz-3top1ya7"},"source":["### Deep neural networks\n","\n","Deep neural nets are more complex neural networks in which the hidden layers perform more functions than standard neural networks. We will now explore some aproaches to deep neural nets."]},{"cell_type":"markdown","metadata":{"id":"1TtmKAsG4DDA"},"source":["#### Convolutional neural network (CNN)\n","\n","In CNNs, mathematical convolutions are used on the input to compute the output. Each layer of the network applies different filters and combines their results."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HUfc6Rch8ZvB","executionInfo":{"elapsed":9125,"status":"ok","timestamp":1623832261614,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"e82fa17b-b5a8-4840-f20f-f9e3934802ff"},"source":["def create_cnn():\n","    # Add an Input Layer\n","    input_layer = layers.Input((70, ))\n","\n","    # Add the word embedding Layer\n","    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n","    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n","\n","    # Add the convolutional Layer\n","    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n","\n","    # Add the pooling Layer\n","    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n","\n","    # Add the output Layers\n","    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n","    output_layer1 = layers.Dropout(0.25)(output_layer1)\n","    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n","\n","    # Compile the model\n","    model = models.Model(inputs=input_layer, outputs=output_layer2)\n","    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n","    \n","    return model\n","\n","classifier = create_cnn()\n","accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n","print(\"CNN, Word Embeddings\",  accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["235/235 [==============================] - 8s 24ms/step - loss: 0.6402\n","CNN, Word Embeddings 0.5212\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eMquwnvT5BLl"},"source":["#### Recurrent neural network: LSTM\n","\n","Unlike feed forward neural nets, the activation outputs of the recurrent neural network propagate in both directions.This creates loops in the network architecture which acts as a memory state for neurons."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9SoRJvx8uCp","executionInfo":{"elapsed":27443,"status":"ok","timestamp":1623832426667,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"a85027a7-8398-474e-ae6c-ab8a66c1c9d2"},"source":["def create_rnn_lstm(): \n","    # Add an Input Layer\n","    input_layer = layers.Input((70, ))\n","\n","    # Add the word embedding Layer\n","    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n","    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n","\n","    # Add the LSTM Layer\n","    lstm_layer = layers.LSTM(100)(embedding_layer)\n","\n","    # Add the output Layers\n","    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n","    output_layer1 = layers.Dropout(0.25)(output_layer1)\n","    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n","\n","    # Compile the model\n","    model = models.Model(inputs=input_layer, outputs=output_layer2)\n","    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n","    \n","    return model\n","\n","classifier = create_rnn_lstm()\n","accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n","print(\"RNN-LSTM, Word Embeddings\",  accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["235/235 [==============================] - 24s 80ms/step - loss: 0.6549\n","RNN-LSTM, Word Embeddings 0.5212\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F8UdQLQT6Arc"},"source":["#### Gated recurrent units (GRU)\n","\n","Another type of recurrent neural network."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PeXvn1h99Q5Y","executionInfo":{"elapsed":21634,"status":"ok","timestamp":1623832531610,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"f1891976-0118-47a1-b137-d18b147b4c54"},"source":["def create_rnn_gru():\n","    # Add an Input Layer\n","    input_layer = layers.Input((70, ))\n","\n","    # Add the word embedding Layer\n","    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n","    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n","\n","    # Add the GRU Layer\n","    lstm_layer = layers.GRU(100)(embedding_layer)\n","\n","    # Add the output Layers\n","    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n","    output_layer1 = layers.Dropout(0.25)(output_layer1)\n","    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n","\n","    # Compile the model\n","    model = models.Model(inputs=input_layer, outputs=output_layer2)\n","    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n","    \n","    return model\n","\n","classifier = create_rnn_gru()\n","accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n","print(\"RNN-GRU, Word Embeddings\",  accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["235/235 [==============================] - 19s 68ms/step - loss: 0.6593\n","RNN-GRU, Word Embeddings 0.5212\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"235yD7IH6Xrv"},"source":["#### Bidirectional RNN\n","\n","A GRU layer wrapped by a bidirectional layer."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QKBXodAx9n8x","executionInfo":{"elapsed":40219,"status":"ok","timestamp":1623832612321,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"081f024a-3437-4097-dabb-08a2391eac5c"},"source":["def create_bidirectional_rnn():\n","    # Add an Input Layer\n","    input_layer = layers.Input((70, ))\n","\n","    # Add the word embedding Layer\n","    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n","    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n","\n","    # Add the LSTM Layer\n","    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n","\n","    # Add the output Layers\n","    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n","    output_layer1 = layers.Dropout(0.25)(output_layer1)\n","    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n","\n","    # Compile the model\n","    model = models.Model(inputs=input_layer, outputs=output_layer2)\n","    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n","    \n","    return model\n","\n","classifier = create_bidirectional_rnn()\n","accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n","print(\"RNN-Bidirectional, Word Embeddings\",  accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["235/235 [==============================] - 36s 125ms/step - loss: 0.6658\n","RNN-Bidirectional, Word Embeddings 0.5212\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O5qmBml_6wFq"},"source":["#### Recurrent convolutional neural network\n","\n","A combination of recurrent and convolutional nets."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EACoBAcC97Zv","executionInfo":{"elapsed":8455,"status":"ok","timestamp":1623832663795,"user":{"displayName":"Kai Mansfield","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhyqSL33uR_KgOMm7q6RBqZjoefHGxzVZ3VWGkN=s64","userId":"14979026357472685818"},"user_tz":-60},"outputId":"65306088-4c5c-4c4b-8a6a-241f6d1190e9"},"source":["def create_rcnn():\n","    # Add an Input Layer\n","    input_layer = layers.Input((70, ))\n","\n","    # Add the word embedding Layer\n","    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n","    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n","    \n","    # Add the recurrent layer\n","    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n","    \n","    # Add the convolutional Layer\n","    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n","\n","    # Add the pooling Layer\n","    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n","\n","    # Add the output Layers\n","    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n","    output_layer1 = layers.Dropout(0.25)(output_layer1)\n","    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n","\n","    # Compile the model\n","    model = models.Model(inputs=input_layer, outputs=output_layer2)\n","    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n","    \n","    return model\n","\n","classifier = create_rcnn()\n","accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n","print(\"CNN, Word Embeddings\",  accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["235/235 [==============================] - 7s 24ms/step - loss: 0.6514\n","CNN, Word Embeddings 0.5212\n"],"name":"stdout"}]}]}